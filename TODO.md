# TODO

Next: why wasn't ä¸€æ—¥ä¸­ found automatically?
Then: integrate Baidu frequencies

Sometime soon: Unit tests for pipeline steps; add_char_prons is large enough that it needs to be split by language


Topic: final output

* host debug outputs somewhere
* ğ°¡» not displaying correctly; come up with JS test for non-displaying characters?
* å¤šç‘™ and å¤šç‘™æ²³, ç„¡æ²™æ±° and ã”ç„¡æ²™æ±°: should not select words that contain each other like this
* maybe link to wiktionary?
* Number the chapters/purity groups for easier navigation
* continuous (but not intrusive) headers for language and purity group
* output language-specific names of components (BS gives pinyin but not Japanese, etc.)
* Get English keywords for components whose info is taken from ytenx
* Use character normalization to retrieve more BS data for components: ä»->å¾, etc., ç“¶
* Re-do structure: vocab surface, pron, meaning need to be in single divs together so that height changes in sync and make selection easier
* Pastel color-coding for languages on edge of pages; kanji for language printed on side (by printing black strips on each page that combine into a character?)
* show weird characters like ã» somewhere because it's interesting.
* Where to put ç•¥å­— such as é—¨, ã§, æ‰, åœ•, å¹¿K, æœ¨ã‚­, etc.?
* List country characters somewhere
* Sample of Kanbun from each country, with pronunciations
* links to same groups in other languages
* character search functionality
* back matter?
    - citations
    - character index
    - timelines

Topic: groupings


Topic: Mandarin support

* Next: integrate baidu frequencies
    - maybe just for those not in CKIP?
* Crawl wiktionary data for "abbreviated phonetic keyword" and add this info
* Allow but disprefer surnames and old stuff
* Unihan: é†£ should be listed as a variant of ç³–, which is the standard form
    - similarly, the ç³– entry has variant é¤¹ but not é†£é¥„ç³›ğ¥¹¥
* Why is å¤©æ–¹å¤œè­š used twice?
* nÇi nai listed twice because it represents two different pronunciations
    - also ä¼¯ä¼¯
* messed up pron formatting: å¥½å¿ƒå€’åšäº†é©¢è‚è‚º hÇo xÄ«n dÃ o zuÃ² le lu:2 gÄn fÄ“i
* parse out classifiers for vocab, remove pronunciation or put in ruby text: ear/CL:éš»|åª[zhi1],å€‹|ä¸ª[ge4],å°|å¯¹[dui4]/handle (on a cup)
* zÃ¬ gÄ› r5
* parse out "see": see ç¾Šæ‹[yang2 guai3]
* Differentiate baidu and wendu
* Get ä¸‰ä½“ word list
    - simp to trad conversion
    - how to present? show chapter 1 vocab with rest of book as backup?
* Possibly provide middle Chinese pronunciations
    - mostly in Ytenx
* Possibly provide old Chinese pronunciations
    - download Baxter/Sagart data

Topic: Japanese Support

* Why does åŸ¼ come first in its group? It has no example words.
* investigate kun-yomi with stars (ã‚€ãª*ã€ã‚€ã­); maybe sort to end? Maybe change formatting?
* investigate char/prons with missing example words
* Consider getting rid of (obsc) and (arch) words (è²æ²» is in output!)
    - generally, meaning fields probably need to be parsed better
* JpAligner not aligning æ¿å¡€ (itabei) because it uses kun'yomi or ç…‰ç“¦å¡€ (rengabei) because it uses non-joyo chars.
    - Sometimes much better examples can be found by allowing kun-yomi; the advantage of using non-joyo chars is more debatable, though
* æ–‡å­—ã‚‚ã‚“ã˜ is a serious problem! Need to list ã‚‚ã˜ pronunciation
    - same with æ—¥æœ¬èª ã«ã£ã½ã‚“ã”
    - can grab same word marked with (P), which means common in EDICT
* Char notes: kokuji section, å‹• is also kokuji but has a phonetic component
* track down unfound Baxter/Sagart data for 125 components
* Would be nice to have Japanese names for components
* More example compounds would actually be quite nice, particularly for the more common characters
* Would be nice to be able to sort ã‚­ãƒ§ã‚¦ãƒ€ã‚¤ before ã‚±ã‚¤ãƒ†ã‚¤ (need frequency of readings to do this)
* move POS into separate field in vocab (is currently part of definition).
* What is "/(P)" in definition field?
* possibly output kun'yomi definitions
* possibly get Japanese glyph alternatives for components

Topic: Korean Support

* There is apparently a hanja test Koreans can take, and 1geup has 3500 hanja
* Source keywords
* Load libhangul data, align to get readings
* Short section on gukja, like ones that include hangeul, etc.
    - https://en.wiktionary.org/wiki/Category:Korean-only_CJKV_Characters
* Note somewhere: interesting that hundok for some characters like ç•œ and å®‰ contain the eumdok
* possibly interesting: https://ko.wiktionary.org/wiki/%EC%82%AC%EC%9A%A9%EC%9E%90:Catagorae/%EC%98%81%EC%96%B4_%ED%95%9C%EA%B5%AD%EC%96%B4_%EB%B2%95%EB%A5%A0%EC%9A%A9%EC%96%B4

Topic: Vietnamese Support


* mark phonetic loans explicitly
* Get word frequency list
* Get han tu spellings for words
    - hanviet: http://web.archive.org/web/20060218090101/http://perso.wanadoo.fr/dang.tk/langues/hanviet.htm
    - try extracting from Wiktionary?
        - Doing now! Save script somewhere...
    - Winvnkey has some great data! Particularly TuPhuc-HanViet.txt, which I haven't found anywhere else.
    - DDB
* Possibly an appendix on nom characters
* vnedict: http://www.denisowski.org/Vietnamese/vnedict.txt
* zetamu: https://web.archive.org/web/20101022085207/http://zung.zetamu.com/Hantu/hantu_index.html
* nomfoundation.org
    - use is explicitly listed as free!
    - will require lots of work to make it usable
* How to find modern words, though?
* short intro to chu nom (common words like I, is, you, etc.)
* Old/Middle Vietnamese pronunciations?

## Tasks

* Convert Jun Da frequency list to traditional characters so we have traditional frequency list (roughly)
* Character lists (grade level, newspaper standard, etc.)
    - https://en.wikisource.org/wiki/Translation:List_of_Frequently_Used_Characters_in_Modern_Chinese#Inferior_frequently_used_characters
    - https://web.archive.org/web/20160404231631/http://resources.publicense.moe.edu.tw/dict_reviseddict_download.html
    - Kanken
* Investigate https://github.com/cburgmer/cjklib
    - website is long gone, project no longer maintained, but it looks pretty comprehensive!
    - Can we revive/modernize it? May be a better data source!
* Investigate http://www.lrec-conf.org/proceedings/lrec2012/pdf/306_Paper.pdf
    - lists some good resources
    - they don't share their results :(
* Investigate https://humanum.arts.cuhk.edu.hk/Lexis/lexi-mf/
    - seems they have a very permissive license (used in Wiktionary), but the content is not downloadable as a single file. This resource is truly amazing; if only it were more easily accessible!
* Investigate Wiktionary
    - has become much more extensive in recent years!
    - sino-xenic compound cognates are already there (see https://en.wiktionary.org/wiki/%E4%BA%94%E7%A9%80)
    - lua error on some pages?
        - example: https://en.wiktionary.org/wiki/%E4%BA%BA
        - tracker: https://phabricator.wikimedia.org/T165935
    - can/should I contribute directly to wiktionary?
        - seems like data is very hit and miss; äºº article for Korean contains many compounds. Check page for äººåŠ›è»Š and it doesn't have a sino-xenic descendants section ğŸ¤”
    - would it still be valuable to create my own database?
        - probably, if nothing else we can find what wiktionary is missing
* Investigate OpenCC
    - https://github.com/BYVoid/OpenCC/blob/master/data/dictionary


## Questions/Needs

* Legal character classifications (grade levels, Jouyou, etc.)
* DB for on readings by period (go-on, kan-on, tou-on, etc.)
* DB for historical kun'yomi
    - (very) partial dataset: https://www.bunka.go.jp/kokugo_nihongo/sisaku/joho/joho/kijun/naikaku/gendaikana/huhyo_i.html
* historical spellings, with wi, we, ye, etc. for Japanese

## Ideas

* Generate shiritori sequence for all characters in a group

## Issues to Open

* Make the on'yomi parser slightly less accommodating and then report misspellings to Unihan
* poetry: script aliases like for npm; test-all = `poetry run pre-commit run --all-files`, etc.
* Unihan does not link éºº and éºµ as variants (Wikipedia correctly shows trad/simp/sinjitai)
* Unihan does not indicate where okurigana in a kun'yomi begin
* Would be great if unihan-etl (or better yet, Unihan itself!) structurized the (traditional variant of X), (non-classical variant of X), (same as X), etc. in the `kDefinition` field.
    - ã‘¶ is listed as the traditional variant of ã¹, but the Mandarin pronunciations are different! The kDefinition field notes that it's treated as a variant of ä»¡, which does have the same pronunciation.
* Unihan-etl doesn't provide the new kStrange field?
* Unihan entries å½™å½šğ¢‘¥ do not mention each other as variants
* Really wish unihan had kyuujitai/sinjitai links; ç¶  links to ç·‘ only in the jinmeiyo kanji field.
* Unihan: relationship between é§„ and é¦± not given; between ç“¶ and ç”, too (sinjitai/kyuujitai)
* Unihan: relationship between æ ƒğ£œœæ«” not given
* Unihan: missing relationship between é¡é¡”, åŒ‚åŒˆ, å€¶ä¿± (from ytenx JihThex),
* comment on Wikipedia about missing syllables and umlauts in pronunciations for CKIP frequency data
* Unihan: kKoreanName messed up hangeul browser display (ì¸ëª…ìš© í•œì)
* Unihan: kGradeLevel only goes through grade 6, but characters are also learned in middle school, so the data is incomplete (missing like 2700 characters)
* Unihan: Wikipedia says å¸¸ç”¨å­—å­—å½¢è¡¨ should have 4762 characters, kHKGlyph has 4824. Could be an issue with variants being included, but really only one glyph should be marked.
* Unihan: the data I gathered could be integrated usefully. The okurigana info in Joyo, old spellings, recently relicensed Baxter-Sagart reconstructions, phonetic components, etc.
* Unihan: would be nice to list kokuji/gukja/chu nom
* Unihan/Ytenx: semantic variant for ç ² is not ç¤®
* Unihan: many characters (æ›²å¸¶æŒ¾, etc.) are set to simplified variant of themselves
* Datapackage: no helpful error message printed when a field cannot be cast to the type in the schema. `validate` only checks that schema definition can be parsed, not that data conforms to it.
    - `package.get_resource('kengdic').read(keyed=True)` gives `datapackage.exceptions.CastError: There are 1 cast errors (see exception.errors) for row "2"`, but does not say what the cast error was (or which column). Had to edit schema.py to print out the actual errors (which were informative!)

## Dev Tools
* integrate darglint and/or pydocstyle to help keep documentation together
* Try rope for refactoring
* Dockerfile
* Faster-to-load format than JSON? Is pickle better?

## Inspirations

* hanzidb.org/about
* hanzicraft.com/about
* https://github.com/jsksxs360/Hanzi
* https://github.com/proycon/hanzigrid
